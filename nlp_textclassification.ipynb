{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnkeZ+t2yvN+oXVGPpFnqW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cutie-tee/nlp_project/blob/main/nlp_textclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Data"
      ],
      "metadata": {
        "id": "KIxsaaHf5pp4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fSQLZyD33I3N",
        "outputId": "dcd5f4bf-280e-43e0-aab2-ad8aad30fe7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['0\\tdonald trump sends out embarrassing new year‚s eve message; this is disturbing'], dtype='object')\n",
            "Column names: ['0\\tdonald trump sends out embarrassing new year‚s eve message; this is disturbing']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "#Load the dataset\n",
        "data =pd.read_csv('/content/training_data_lowercase.csv')\n",
        "#print(data.head())\n",
        "print(data.columns)\n",
        "#print(data.head())\n",
        "\n",
        "# column names\n",
        "column_names = data.columns.tolist()\n",
        "print(\"Column names:\", column_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the CSV with tab delimiter\n",
        "data = pd.read_csv('/content/training_data_lowercase.csv', delimiter='\\t')\n",
        "\n",
        "# Rename the columns for clarity\n",
        "data.columns = ['label', 'text']\n",
        "# Display the first few rows to confirm\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "0ztefrpXARk_",
        "outputId": "5f538f1d-8e72-4549-8bf9-824fdffe35eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                               text\n",
            "0      0  drunk bragging trump staffer started russian c...\n",
            "1      0  sheriff david clarke becomes an internet joke ...\n",
            "2      0  trump is so obsessed he even has obama‚s name ...\n",
            "3      0  pope francis just called out donald trump duri...\n",
            "4      0  racist alabama cops brutalize black boy while ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing ( Cleaning& Tokenisation)"
      ],
      "metadata": {
        "id": "M06rjypV7cao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean the text to convert to lowercase, remove punctuation, whitespace , stop words, toeksnising andlemmatising\n",
        "\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define stop words and preprocessing function\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize and remove stopwords\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to the 'text' column\n",
        "data['cleaned_text'] = data['text'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "7e0cE65D7j5b",
        "outputId": "0f481851-ec83-4d27-ff03-3caf2d9aab6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting Test to Numerical Features"
      ],
      "metadata": {
        "id": "RszNwjNIA7ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(data['cleaned_text'])\n",
        "y = data['label']  # Assuming 'label' column is already numeric; if not, convert it with LabelEncoder\n"
      ],
      "metadata": {
        "id": "hMd8RatBA_j_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Vectorize Text Data with TF-IDF"
      ],
      "metadata": {
        "id": "0vFj5CHlBGUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Limit features for efficiency\n",
        "X = vectorizer.fit_transform(data['cleaned_text'])\n",
        "y = data['label']  # Assuming 'label' is the target variable\n"
      ],
      "metadata": {
        "id": "xPSOcyktB3qW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Test Split"
      ],
      "metadata": {
        "id": "qjBG32hLBzYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "wBep4jw-BF3-"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}